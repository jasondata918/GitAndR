---
title: "Basic Models"
subtitle: "WLS, Logistic and Trees"
author: "JASON RHEE"
date: "February 17, 2023"
output:
  word_document:
   toc: true
   toc_depth: 2
---

```{r global_options}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## Setup

This analysis will be done with the **Hitters{ISLR}** baseball player data set, using AtBat, Hits, Walks, PutOuts, Assists and HmRun as predictors and player **Salary** as the outcome variable. Also, set the `options(scipen = 4)` to minimize the use of scientific notation.

```{r}
# Prep work 
library(ISLR) # Contains the Hitters data set
options(scipen = 4)
```

Familiarize yourself with the Hitters data set by entering the commands below in the R Console window, but NOT in the R Markdown file. Inspect the data and the description of each predictor, to familiarize yourself with the data

`?Hitters`
`View(Hitters)`

 Let's start with an OLS model, which you will then test for heteroskedasticity.

```{r}

# The Hitters data set has several records with missing data, let's remove them

Hitters <- na.omit(Hitters) 

# We now fit an OLS model to start with

fit.ols <- lm(Salary ~ AtBat + Hits + Walks + 
                       PutOuts + Assists + HmRun, 
                       data = Hitters)

summary(fit.ols) # Check it out

# As the output shows, there are 4 significant predictors: AtBat, Hits, Walks and PutOuts, and 2 non-significant predictors: Assists and HmRun.
```

## 1. Heteroskedasticity Testing

1.1 Inspect the residuals visually for heteroskedasticity. To do this, display the first residual `plot()` for **fit.ols** using the parameter `which = 1`.
   
```{r}
plot(fit.ols, which = 1)
```

1.2 Then load the **{lmtest}** library and conduct a **Breusch-Pagan** test for Heteroskedasticity for the **fit.ols** model above, using the `bptest()` function.

```{r}
library(lmtest) 
bptest(fit.ols, data = Hitters)
```

1.3 Is there a problem with Heteroskedasticity? Why or why not? In your answer, please refer to **both**, the residual plot and the BP test.  
    
The Residuals Vs Fitted model shows that the error variance is not evenly distributed and appears to fade-out as predictive value gets larger. This indicates that the errors are heteroskedastic. 

The Breusch-Pagan test p-value of 0.017, which means that residual regression is significant and there is heteroskedasticity.


## 2. Weighted Least Squares (WLS) Model

2.1 Let's set up the parameters of the WLS model. Let's start by using the `fitted()` function to extract the fitted (i.e., predicted) values from the **fit.ols** object created above and store the results in a vector object named **fitted.ols**. 

```{r}
fitted.ols <- fitted(fit.ols)
```

2.2 Then, use the `abs()` and `residuals()` functions, compute the absolute value of the residuals from the OLS model **fit.ols** and store the results in a vector object named **abs.res**. Then use the `cbind()` function to list the **fitted.ols** and **abs.res** values side by side for the first 10 records (tip: add the index `[1:10, ]` after the function to list only the first 10 rows and all columns)

```{r}
abs.res <- abs(residuals(fit.ols)) 
cbind(fitted.ols, abs.res)[1:10, ]
```

2.3 Now that you have two vectors, one with the absolute value of the residuals and one with the predicted values of the outcome variable Salary, fit an `lm()` model using **fitted.ols** as a predictor vector for the absolute value of the residuals in **abs.res** as the outcome. To check your results, display the first 10 rows of the `fitted()` values of **lm.abs.res** (tip: again, use the `[1:10]` index after the function)

**Technical tip:** Because you are using one data vector to predict another data vector, you don't need the `data =` parameter. You only need the `data =` parameter when your variables are columns in a data frame.

```{r}
lm.abs.res <- lm(abs.res ~ fitted.ols) 
fitted(lm.abs.res)[1:10]
```

the difference between **fitted.ols**, **abs.res** and **fitted(lm.abs.res)

```{r}
# fitted.ols is a vector containing the predicted values of the OLS model

# abs.res is a vector containing the absolute values of the residuals from fitted.ols

# fitted(lm.abs.res) is a vector containing the predicted values of the absolute values of the errors. We could use abs.res for the weight vector in WLS, and some methods do that, but the problem is that the actual values of the residuals vary all over the place. We use the fitted values instead to use a smoother set of weights (from he straight line of the regression)
```

2.4 To visualize the lm.abs.res regression line, plot the **fitted.ols** vector against the **abs.res** vector. Then draw a red line using the `abline()` function for the **lm.abs.res** regression object.

**Technical Note:** Notice that I use the `fig.width` and `fig.height` attributes in the `{r` code chunk header to define the size of the plots in inches.

```{r fig.width = 8, fig.height = 6}
plot(fitted.ols, abs.res) 
abline(lm.abs.res, col="red") 
```

2.5 Specify and run the **WLS** regression model. First, create a weight vector named **wts** equal to the inverse squared predicted values of **lm.abs.res** (tip: use `wts <- 1 / fitted(lm.abs.res) ^ 2`). To check things, display the first 10 rows of the **wts** vector.

```{r}
wts <- 1/fitted(lm.abs.res)^2
wts[1:10]
```

Then fit the WLS regression model using the same predictors you used in **ols.fit**, but using **wts** for the `weights =` parameter. Name this regression object **wls.fit**. Display the summary results.

While we are at it, also fit a similar weighted GLM model (**WGLM**), by using the `glm()` function and the exact same specification you used in the `lm()` function, and store the results in an object named **fit.wglm**. Then display the `summary()` results for the WGLM. 

```{r}
fit.wls <- lm(Salary ~ AtBat + Hits + Walks + PutOuts + Assists + HmRun, 
              data = Hitters, weights = wts)
summary(fit.wls)

fit.wglm <- glm(Salary ~ AtBat + Hits + Walks + PutOuts + Assists + HmRun, 
                data = Hitters, weights = wts)
summary(fit.wglm)

```

2.6 Similarities an differences between the OLS, WLS and WGLM model

By comparing the R-Squared for OLS model of 0.311 and WLS model of 0.175, we can conclude that the R-Squared went down. This explains that the R-Squared for WLS is not exactly the proportion of explained variance, but the weighted variance. We can not compare R-Squares. 

From WLS test confirms that it has less variance than OLS, since OLS residuals are heteroskedastic. We also found out that 2 non-significant predictors in OLS 'Assists' and 'Home Run' become significant in WLS and other 4 significant predictors remained same. 

The WLS and WGLM assumptions hold in full, which means that the WGLM model yields the exact same result mathematically. The WLS model reports the R-square and F-test and The WGLM reports 2LL fits statistics and the residual deviance is 609.


## 3. Logistic Regression

3.1 Download the **myopia.csv** file to your working directory. Then read it using `read.table()` with the parameters `header = T, row.names = 1, sep = ","`. Store the data set in an object named **myopia**. 

Please review the data set documentation at: https://rdrr.io/cran/aplore3/man/myopia.html

please note that **myopic** is coded as 1 (Yes), 0 (No)

For sanity check, list the first 10 rows and 8 columns of this data set.

```{r}
myopia <- read.table("myopia.csv", header = T, row.names = 1, sep = ",") 
myopia[1:10, 1:8]
```

3.2 Fit a logistic model to predict whether a child is **myopic**, using `age + female + sports.hrs + read.hrs + mommy + dadmy` as predictors. Use the parameters `family = "binomial"(link = "logit")` to specify the Logistic model. Store the results in an object named **myopia.logit**. Display the `summary()` results. Then display the `summary()` results.

```{r}
myopia.logit <- glm(myopic ~ age + female + sports.hrs + read.hrs + mommy + dadmy, 
                    family = "binomial"(link = "logit"), data = myopia) 
summary(myopia.logit)
```

3.3 For interpretation purposes, display the log-odds alongside the odds. Use the `coef()` function to extract the log-odds coefficients from **myopia.logit** and save them in a vector object named **log.odds**. Then use the `exp()` function to convert the log-odds into odds and store the results in a vector object named **odds**. 

```{r}
log.odds <- coef(myopia.logit)
odds <- exp(log.odds)
```

3.4 Finally, list the log-odds and odds side by side using the `cbind()` function. Name the columns as shown in the display below. Once you test that your `cbind()` function is working correctly, embed the function inside the `print()` function with the parameter `digits = 2` to get a more compact display.

```{r}
print(cbind("Log-Odds" = log.odds, "Odds" = odds), 
      digits = 2)
```
 
3.5 The log-odds and odds effects of **read.hrs** and **mommy**.

Both 'read.hrs' and 'mommy' are significant. Holding everything else constant, on average for each additional hour of reading per week, the log-odd of developing myopia would increase by 0.799 and the odd increase by a factor of 2.2.

Holding everything else constant, on average, if the mother is myopic, the log-odds of the child developing myopia increase by 2.7937 and the odd increase by a factor of 19.0.



## 4. Decision Trees

**4.1 Regression Tree**. Load the **{tree}** library. Then fit a regression tree with the same specification as the regression model **ols.fit** above. Use the `tree()` function and save the results in an object named **fit.tree.salary**. Then plot the tree using the `plot()` and `text()` functions (use the `pretty = 0` parameter). Also use the `title()` function to title you tree diagram **Baseball Salaries Regression Tree**.

```{r fig.width = 10, fig.height = 8}
library(tree)
fit.tree.salary <- tree(Salary ~ AtBat + Hits + Walks + PutOuts + Assists + HmRun,
                        data = Hitters)
plot(fit.tree.salary) 
text(fit.tree.salary, pretty = 0) 
title("Baseball Salaries Regression Tree")
```

4.2 Classification Tree. 

Before you start, check the `class()` of the `myopia$myopic` variable and you will notice that it is an integer, not a factor (categorical) variable. This works fine in a Logistic regression model, but a factor outcome variable gives you better visual displays in classification trees. Let's create the corresponding factor variable with `myopia$myopic.f <- as.factor(myopia$myopic)`. Notice that we are renaming the outcome variable so that we don't disturb the original variables. To be certain that the vector was converted from text to factor, list the `class()` of the `myopia$myopic.f` vector.

```{r}
class(myopia$myopic)
myopia$myopic.f <- as.factor(myopia$myopic) 
class(myopia$myopic.f)
```

Fit a classification tree model using the same specification as the Logistic model **myopia.logit**, but using `myopic.f` as the outcome variable. Use the `tree()` function and save the results in an object named **fit.tree.myopia**. Then plot the tree using the `plot()` and `text()` functions (use the `pretty = 0` parameter). Also use the `title()` function to title you tree diagram **Myopia Classification Tree**.

```{r fig.width = 10, fig.height = 8}
fit.tree.myopia <- tree(myopic.f ~ age + female + sports.hrs + read.hrs + mommy + dadmy, 
                        data = myopia) 
plot(fit.tree.myopia) 
text(fit.tree.myopia, pretty = 0) 
title("Myopia Classification Tree")
```
